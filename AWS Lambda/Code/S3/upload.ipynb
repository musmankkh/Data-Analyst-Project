{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8d3522b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import boto3\n",
    "from botocore.exceptions import ClientError\n",
    "import os\n",
    "from pathlib import Path\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c09e277",
   "metadata": {},
   "outputs": [],
   "source": [
    "BUCKET_NAME = 'soccer-database-project' \n",
    "REGION = 'us-east-1'\n",
    "CSV_DIRECTORY = r'D:\\Intern\\AWS Lambda\\Dataset' \n",
    "S3_FOLDER = 'raw-dataset'\n",
    "LAMBDA_FUNCTION_NAME = 'data-cleaning-function'  # UPDATE THIS\n",
    "AWS_ACCOUNT_ID = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb15eed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "AWS_SECRET_ACCESS_KEY=''\n",
    "AWS_ACCESS_KEY_ID='' \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8672f9ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_aws_clients(aws_access_key=AWS_ACCESS_KEY_ID, aws_secret_key=AWS_SECRET_ACCESS_KEY, region='us-east-1'):\n",
    "    \"\"\"Initialize AWS clients for S3 and Lambda\"\"\"\n",
    "    if aws_access_key and aws_secret_key:\n",
    "        s3 = boto3.client('s3', aws_access_key_id=aws_access_key, \n",
    "                         aws_secret_access_key=aws_secret_key, region_name=region)\n",
    "        lambda_client = boto3.client('lambda', aws_access_key_id=aws_access_key,\n",
    "                                    aws_secret_access_key=aws_secret_key, region_name=region)\n",
    "    else:\n",
    "        s3 = boto3.client('s3', region_name=region)\n",
    "        lambda_client = boto3.client('lambda', region_name=region)\n",
    "    \n",
    "    return s3, lambda_client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fd5b0a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_s3_bucket(bucket_name, aws_access_key=AWS_ACCESS_KEY_ID, \n",
    "                    aws_secret_key=AWS_SECRET_ACCESS_KEY, region='us-east-1'):\n",
    "    \"\"\"Create S3 bucket if it doesn't exist\"\"\"\n",
    "    s3, _ = get_aws_clients(aws_access_key, aws_secret_key, region)\n",
    "    \n",
    "    try:\n",
    "        s3.head_bucket(Bucket=bucket_name)\n",
    "        print(f\"‚úì Bucket '{bucket_name}' already exists\")\n",
    "        return True\n",
    "    except ClientError as e:\n",
    "        error_code = e.response['Error']['Code']\n",
    "        if error_code == '404':\n",
    "            try:\n",
    "                if region == 'us-east-1':\n",
    "                    s3.create_bucket(Bucket=bucket_name)\n",
    "                else:\n",
    "                    s3.create_bucket(\n",
    "                        Bucket=bucket_name,\n",
    "                        CreateBucketConfiguration={'LocationConstraint': region}\n",
    "                    )\n",
    "                print(f\"‚úì Successfully created bucket '{bucket_name}' in region '{region}'\")\n",
    "                return True\n",
    "            except ClientError as create_error:\n",
    "                print(f\"‚úó Error creating bucket: {create_error}\")\n",
    "                return False\n",
    "        else:\n",
    "            print(f\"‚úó Error checking bucket: {e}\")\n",
    "            return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4de411e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_csv_to_s3(file_path, bucket_name, s3_key=None, aws_access_key=AWS_ACCESS_KEY_ID, \n",
    "                     aws_secret_key=AWS_SECRET_ACCESS_KEY, region='us-east-1'):\n",
    "    \"\"\"Upload a single CSV file to S3\"\"\"\n",
    "    \n",
    "    # Initialize S3 client\n",
    "    if aws_access_key and aws_secret_key:\n",
    "        s3 = boto3.client('s3', aws_access_key_id=aws_access_key,\n",
    "                         aws_secret_access_key=aws_secret_key, region_name=region)\n",
    "    else:\n",
    "        s3 = boto3.client('s3', region_name=region)\n",
    "    \n",
    "    # If s3_key not provided, use the filename\n",
    "    if s3_key is None:\n",
    "        s3_key = os.path.basename(file_path)\n",
    "    \n",
    "    try:\n",
    "        # Upload the file\n",
    "        s3.upload_file(file_path, bucket_name, s3_key)\n",
    "        print(f\"‚úì Successfully uploaded {os.path.basename(file_path)} to s3://{bucket_name}/{s3_key}\")\n",
    "        return True\n",
    "    except FileNotFoundError:\n",
    "        print(f\"‚úó Error: File {file_path} not found\")\n",
    "        return False\n",
    "    except ClientError as e:\n",
    "        print(f\"‚úó Error uploading {file_path}: {e}\")\n",
    "        return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "845670b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_multiple_csvs(directory, bucket_name, s3_folder='', aws_access_key=AWS_ACCESS_KEY_ID,\n",
    "                        aws_secret_key=AWS_SECRET_ACCESS_KEY, region='us-east-1'):\n",
    "    \"\"\"Upload all CSV files from a directory to S3\"\"\"\n",
    "    \n",
    "    results = {'success': 0, 'failed': 0, 'files': []}\n",
    "    \n",
    "    # Get all CSV files in directory\n",
    "    csv_files = list(Path(directory).glob('*.csv'))\n",
    "    \n",
    "    if not csv_files:\n",
    "        print(f\"No CSV files found in {directory}\")\n",
    "        return results\n",
    "    \n",
    "    print(f\"Found {len(csv_files)} CSV file(s) to upload\\n\")\n",
    "    \n",
    "    for csv_file in csv_files:\n",
    "        # Construct S3 key with optional folder\n",
    "        s3_key = f\"{s3_folder}/{csv_file.name}\" if s3_folder else csv_file.name\n",
    "        \n",
    "        success = upload_csv_to_s3(\n",
    "            str(csv_file), \n",
    "            bucket_name, \n",
    "            s3_key,\n",
    "            aws_access_key,\n",
    "            aws_secret_key,\n",
    "            region\n",
    "        )\n",
    "        \n",
    "        if success:\n",
    "            results['success'] += 1\n",
    "            results['files'].append(str(csv_file))\n",
    "        else:\n",
    "            results['failed'] += 1\n",
    "    \n",
    "    print(f\"\\n--- Upload Summary ---\")\n",
    "    print(f\"Total: {len(csv_files)} | Success: {results['success']} | Failed: {results['failed']}\")\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a230506d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_lambda_permission(lambda_function_name, bucket_name, account_id, \n",
    "                         aws_access_key=AWS_ACCESS_KEY_ID, aws_secret_key=AWS_SECRET_ACCESS_KEY,\n",
    "                         region='us-east-1'):\n",
    "    \"\"\"Add permission for S3 to invoke Lambda\"\"\"\n",
    "    _, lambda_client = get_aws_clients(aws_access_key, aws_secret_key, region)\n",
    "    \n",
    "    statement_id = f'S3InvokePermission-{bucket_name}'\n",
    "    \n",
    "    try:\n",
    "        # Remove existing permission if it exists\n",
    "        try:\n",
    "            lambda_client.remove_permission(\n",
    "                FunctionName=lambda_function_name,\n",
    "                StatementId=statement_id\n",
    "            )\n",
    "            print(f\"‚úì Removed existing permission\")\n",
    "        except ClientError:\n",
    "            pass  # Permission doesn't exist, continue\n",
    "        \n",
    "        # Add new permission\n",
    "        lambda_client.add_permission(\n",
    "            FunctionName=lambda_function_name,\n",
    "            StatementId=statement_id,\n",
    "            Action='lambda:InvokeFunction',\n",
    "            Principal='s3.amazonaws.com',\n",
    "            SourceArn=f'arn:aws:s3:::{bucket_name}',\n",
    "            SourceAccount=account_id\n",
    "        )\n",
    "        print(f\"‚úì Added Lambda permission for S3 to invoke function\")\n",
    "        return True\n",
    "        \n",
    "    except ClientError as e:\n",
    "        print(f\"‚úó Error adding Lambda permission: {e}\")\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "738feedf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def configure_s3_trigger(bucket_name, lambda_function_name, region, account_id,\n",
    "                        prefix='raw-dataset/', \n",
    "                        aws_access_key=AWS_ACCESS_KEY_ID, aws_secret_key=AWS_SECRET_ACCESS_KEY):\n",
    "    \"\"\"Configure S3 to trigger Lambda on file upload\"\"\"\n",
    "    s3, _ = get_aws_clients(aws_access_key, aws_secret_key, region)\n",
    "    \n",
    "    lambda_arn = f'arn:aws:lambda:{region}:{account_id}:function:{lambda_function_name}'\n",
    "    \n",
    "    notification_configuration = {\n",
    "        'LambdaFunctionConfigurations': [\n",
    "            {\n",
    "                'Id': 'TriggerLambdaOnUpload',\n",
    "                'LambdaFunctionArn': lambda_arn,\n",
    "                'Events': ['s3:ObjectCreated:*'],\n",
    "                'Filter': {\n",
    "                    'Key': {\n",
    "                        'FilterRules': [\n",
    "                            {'Name': 'prefix', 'Value': prefix}\n",
    "                        ]\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        s3.put_bucket_notification_configuration(\n",
    "            Bucket=bucket_name,\n",
    "            NotificationConfiguration=notification_configuration\n",
    "        )\n",
    "        print(f\"‚úì Successfully configured S3 trigger for Lambda\")\n",
    "        print(f\"  ‚Üí Watching: s3://{bucket_name}/{prefix}\")\n",
    "        print(f\"  ‚Üí Triggers: {lambda_function_name}\")\n",
    "        return True\n",
    "        \n",
    "    except ClientError as e:\n",
    "        print(f\"‚úó Error configuring S3 trigger: {e}\")\n",
    "        print(f\"  Make sure Lambda permissions are set first!\")\n",
    "        return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "109f80e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def verify_trigger_configuration(bucket_name, aws_access_key=AWS_ACCESS_KEY_ID, \n",
    "                                aws_secret_key=AWS_SECRET_ACCESS_KEY, region='us-east-1'):\n",
    "    \"\"\"Verify the trigger is configured correctly\"\"\"\n",
    "    s3, _ = get_aws_clients(aws_access_key, aws_secret_key, region)\n",
    "    \n",
    "    try:\n",
    "        response = s3.get_bucket_notification_configuration(Bucket=bucket_name)\n",
    "        \n",
    "        if 'LambdaFunctionConfigurations' in response and response['LambdaFunctionConfigurations']:\n",
    "            print(f\"\\n‚úì Active S3 Event Notifications:\")\n",
    "            for config in response['LambdaFunctionConfigurations']:\n",
    "                print(f\"  ‚Üí ID: {config.get('Id')}\")\n",
    "                print(f\"  ‚Üí Lambda: {config.get('LambdaFunctionArn')}\")\n",
    "                print(f\"  ‚Üí Events: {config.get('Events')}\")\n",
    "                if 'Filter' in config:\n",
    "                    filters = config['Filter'].get('Key', {}).get('FilterRules', [])\n",
    "                    for f in filters:\n",
    "                        print(f\"  ‚Üí {f['Name']}: {f['Value']}\")\n",
    "            return True\n",
    "        else:\n",
    "            print(f\"\\n‚ö† No event notifications configured for bucket '{bucket_name}'\")\n",
    "            return False\n",
    "            \n",
    "    except ClientError as e:\n",
    "        print(f\"‚úó Error checking trigger configuration: {e}\")\n",
    "        return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8099737f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_complete_pipeline(bucket_name=BUCKET_NAME, lambda_function_name=LAMBDA_FUNCTION_NAME,\n",
    "                           account_id=AWS_ACCOUNT_ID, region=REGION):\n",
    "    \"\"\"Complete setup: bucket, permissions, and trigger\"\"\"\n",
    "    \n",
    "    print(\"=\" * 70)\n",
    "    print(\"üöÄ AWS Lambda + S3 Pipeline Setup\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # Step 1: Create bucket\n",
    "    print(\"\\n[1/4] Creating S3 bucket...\")\n",
    "    if not create_s3_bucket(bucket_name, region=region):\n",
    "        print(\"‚ùå Setup failed at bucket creation\")\n",
    "        return False\n",
    "    \n",
    "    # Step 2: Add Lambda permission\n",
    "    print(\"\\n[2/4] Adding Lambda permission for S3...\")\n",
    "    if not add_lambda_permission(lambda_function_name, bucket_name, account_id, region=region):\n",
    "        print(\"‚ùå Setup failed at Lambda permission\")\n",
    "        return False\n",
    "    \n",
    "    # Step 3: Configure S3 trigger\n",
    "    print(\"\\n[3/4] Configuring S3 event notification...\")\n",
    "    if not configure_s3_trigger(bucket_name, lambda_function_name, region, account_id):\n",
    "        print(\"‚ùå Setup failed at S3 trigger configuration\")\n",
    "        return False\n",
    "    \n",
    "    # Step 4: Verify\n",
    "    print(\"\\n[4/4] Verifying configuration...\")\n",
    "    verify_trigger_configuration(bucket_name, region=region)\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"‚úÖ Setup Complete!\")\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"\\nüìÅ Upload files to: s3://{bucket_name}/raw-dataset/\")\n",
    "    print(f\"‚ö° Lambda will automatically process them!\")\n",
    "    \n",
    "    return True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f28302ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "üèóÔ∏è  S3 + Lambda Pipeline Configuration\n",
      "======================================================================\n",
      "\n",
      "=== Step 1: Creating S3 Bucket ===\n",
      "‚úì Successfully created bucket 'soccer-database-project' in region 'us-east-1'\n",
      "\n",
      "=== Step 2: Scanning CSV Files ===\n",
      "Found 5 CSV file(s) in 'D:\\Intern\\AWS Lambda\\Dataset':\n",
      "  1. League.csv\n",
      "  2. Player.csv\n",
      "  3. Player_Attributes.csv\n",
      "  4. Team.csv\n",
      "  5. Team_Attributes.csv\n",
      "\n",
      "=== Step 3: Lambda Trigger Setup ===\n",
      "‚ö†Ô∏è  To enable automatic Lambda triggers:\n",
      "1. Update LAMBDA_FUNCTION_NAME = 'data-cleaning-function'\n",
      "2. Update AWS_ACCOUNT_ID = '311353793773'\n",
      "3. Uncomment the setup_complete_pipeline() line below\n",
      "\n",
      "======================================================================\n",
      "üöÄ AWS Lambda + S3 Pipeline Setup\n",
      "======================================================================\n",
      "\n",
      "[1/4] Creating S3 bucket...\n",
      "‚úì Bucket 'soccer-database-project' already exists\n",
      "\n",
      "[2/4] Adding Lambda permission for S3...\n",
      "‚úì Removed existing permission\n",
      "‚úì Added Lambda permission for S3 to invoke function\n",
      "\n",
      "[3/4] Configuring S3 event notification...\n",
      "‚úì Successfully configured S3 trigger for Lambda\n",
      "  ‚Üí Watching: s3://soccer-database-project/raw-dataset/\n",
      "  ‚Üí Triggers: data-cleaning-function\n",
      "\n",
      "[4/4] Verifying configuration...\n",
      "\n",
      "‚úì Active S3 Event Notifications:\n",
      "  ‚Üí ID: TriggerLambdaOnUpload\n",
      "  ‚Üí Lambda: arn:aws:lambda:us-east-1:311353793773:function:data-cleaning-function\n",
      "  ‚Üí Events: ['s3:ObjectCreated:*']\n",
      "  ‚Üí Prefix: raw-dataset/\n",
      "\n",
      "======================================================================\n",
      "‚úÖ Setup Complete!\n",
      "======================================================================\n",
      "\n",
      "üìÅ Upload files to: s3://soccer-database-project/raw-dataset/\n",
      "‚ö° Lambda will automatically process them!\n",
      "\n",
      "‚úì Active S3 Event Notifications:\n",
      "  ‚Üí ID: TriggerLambdaOnUpload\n",
      "  ‚Üí Lambda: arn:aws:lambda:us-east-1:311353793773:function:data-cleaning-function\n",
      "  ‚Üí Events: ['s3:ObjectCreated:*']\n",
      "  ‚Üí Prefix: raw-dataset/\n",
      "\n",
      "=== Step 4: Upload Files to S3 ===\n",
      "\n",
      "üì§ Starting upload...\n",
      "Found 5 CSV file(s) to upload\n",
      "\n",
      "‚úì Successfully uploaded League.csv to s3://soccer-database-project/raw-dataset/League.csv\n",
      "‚úì Successfully uploaded Player.csv to s3://soccer-database-project/raw-dataset/Player.csv\n",
      "‚úì Successfully uploaded Player_Attributes.csv to s3://soccer-database-project/raw-dataset/Player_Attributes.csv\n",
      "‚úì Successfully uploaded Team.csv to s3://soccer-database-project/raw-dataset/Team.csv\n",
      "‚úì Successfully uploaded Team_Attributes.csv to s3://soccer-database-project/raw-dataset/Team_Attributes.csv\n",
      "\n",
      "--- Upload Summary ---\n",
      "Total: 5 | Success: 5 | Failed: 0\n",
      "\n",
      "‚úÖ Successfully uploaded 5 file(s)!\n",
      "‚ö° Lambda function should be processing them now...\n",
      "üìä Check CloudWatch Logs to see processing status\n",
      "\n",
      "======================================================================\n",
      "‚úÖ Script Complete!\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    print(\"=\" * 70)\n",
    "    print(\"üèóÔ∏è  S3 + Lambda Pipeline Configuration\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # Step 1: Create/Verify Bucket\n",
    "    print(\"\\n=== Step 1: Creating S3 Bucket ===\")\n",
    "    bucket_created = create_s3_bucket(\n",
    "        bucket_name=BUCKET_NAME,\n",
    "        region=REGION,\n",
    "        aws_access_key=AWS_ACCESS_KEY_ID,\n",
    "        aws_secret_key=AWS_SECRET_ACCESS_KEY\n",
    "    )\n",
    "    \n",
    "    if not bucket_created:\n",
    "        print(\"‚ùå Failed to create/access bucket. Stopping.\")\n",
    "        exit(1)\n",
    "    \n",
    "    # Step 2: List CSV Files\n",
    "    print(\"\\n=== Step 2: Scanning CSV Files ===\")\n",
    "    csv_files = list(Path(CSV_DIRECTORY).glob('*.csv'))\n",
    "    print(f\"Found {len(csv_files)} CSV file(s) in '{CSV_DIRECTORY}':\")\n",
    "    for i, file in enumerate(csv_files, 1):\n",
    "        print(f\"  {i}. {file.name}\")\n",
    "    \n",
    "    # Step 3: Setup Lambda Trigger (Optional - uncomment to run)\n",
    "    print(\"\\n=== Step 3: Lambda Trigger Setup ===\")\n",
    "    print(\"‚ö†Ô∏è  To enable automatic Lambda triggers:\")\n",
    "    print(f\"1. Update LAMBDA_FUNCTION_NAME = '{LAMBDA_FUNCTION_NAME}'\")\n",
    "    print(f\"2. Update AWS_ACCOUNT_ID = '{AWS_ACCOUNT_ID}'\")\n",
    "    print(\"3. Uncomment the setup_complete_pipeline() line below\\n\")\n",
    "    \n",
    "    # Uncomment this line after updating the config:\n",
    "    setup_complete_pipeline()\n",
    "    \n",
    "    # Or verify existing trigger:\n",
    "    verify_trigger_configuration(BUCKET_NAME, region=REGION)\n",
    "    \n",
    "    # Step 4: Upload Files\n",
    "    print(\"\\n=== Step 4: Upload Files to S3 ===\")\n",
    "    upload_choice = input(\"Do you want to upload CSV files now? (yes/no): \").strip().lower()\n",
    "    \n",
    "    if upload_choice in ['yes', 'y']:\n",
    "        print(\"\\nüì§ Starting upload...\")\n",
    "        results = upload_multiple_csvs(\n",
    "            directory=CSV_DIRECTORY,\n",
    "            bucket_name=BUCKET_NAME,\n",
    "            s3_folder=S3_FOLDER,\n",
    "            aws_access_key=AWS_ACCESS_KEY_ID,\n",
    "            aws_secret_key=AWS_SECRET_ACCESS_KEY,\n",
    "            region=REGION\n",
    "        )\n",
    "        \n",
    "        if results['success'] > 0:\n",
    "            print(f\"\\n‚úÖ Successfully uploaded {results['success']} file(s)!\")\n",
    "            if LAMBDA_FUNCTION_NAME != 'your-lambda-function-name':\n",
    "                print(\"‚ö° Lambda function should be processing them now...\")\n",
    "                print(\"üìä Check CloudWatch Logs to see processing status\")\n",
    "    else:\n",
    "        print(\"\\n‚è≠Ô∏è  Skipping upload. Run upload_multiple_csvs() when ready.\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"‚úÖ Script Complete!\")\n",
    "    print(\"=\" * 70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
